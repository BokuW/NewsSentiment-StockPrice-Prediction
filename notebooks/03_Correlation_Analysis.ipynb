{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce19675",
   "metadata": {},
   "source": [
    "# Import Independencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cceabccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/03_Correlation_Analysis.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob # For sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53c68e",
   "metadata": {},
   "source": [
    "*Project set up importing reusable modules*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ab7ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction' to sys.path for module imports.\n",
      "Project structure setup complete and config.py created/updated.\n",
      "Base Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\n",
      "News Raw Path: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\data\\raw_analyst_ratings.csv\n",
      "Stock Data Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\data\\yfinance_data\n",
      "Tickers to process: ['AAPL', 'AMZN', 'GOOG', 'META', 'NVDA', 'TSLA']\n"
     ]
    }
   ],
   "source": [
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "# importing from src.config\n",
    "def find_project_root(current_path):\n",
    "    \"\"\"\n",
    "    Finds the project root by looking for common project directories.\n",
    "    Assumes project_root contains 'src', 'data', and 'notebooks'.\n",
    "    \"\"\"\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path # Fallback if no specific root found\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables from your src.config\n",
    "from src.config import NEWS_RAW_PATH, STOCK_DATA_DIR, STOCK_TICKERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318112c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Task 3: Correlation Analysis ---\n",
      "\n",
      "--- Loading News Data ---\n",
      "News data loaded successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1407328 entries, 0 to 1407327\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   Unnamed: 0  1407328 non-null  int64 \n",
      " 1   headline    1407328 non-null  object\n",
      " 2   url         1407328 non-null  object\n",
      " 3   publisher   1407328 non-null  object\n",
      " 4   date        1407328 non-null  object\n",
      " 5   stock       1407328 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 64.4+ MB\n",
      "News DataFrame Info (initial load): None\n",
      "\n",
      "First 5 rows of News data (initial load):\n",
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "                        date stock  \n",
      "0  2020-06-05 10:30:54-04:00     A  \n",
      "1  2020-06-03 10:45:20-04:00     A  \n",
      "2  2020-05-26 04:30:07-04:00     A  \n",
      "3  2020-05-22 12:45:06-04:00     A  \n",
      "4  2020-05-22 11:38:59-04:00     A  \n",
      "\n",
      "--- Loading Stock Data (e.g., AMZN) ---\n",
      "Stock data for AMZN loaded successfully.\n",
      "Stock DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 6846 entries, 1997-05-15 to 2024-07-30\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Open          6846 non-null   float64\n",
      " 1   High          6846 non-null   float64\n",
      " 2   Low           6846 non-null   float64\n",
      " 3   Close         6846 non-null   float64\n",
      " 4   Adj_Close     6846 non-null   float64\n",
      " 5   Volume        6846 non-null   int64  \n",
      " 6   Dividends     6846 non-null   float64\n",
      " 7   Stock_Splits  6846 non-null   float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 481.4 KB\n",
      "\n",
      "First 5 rows of Stock data:\n",
      "                Open      High       Low     Close  Adj_Close      Volume  \\\n",
      "Date                                                                        \n",
      "1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000   \n",
      "1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000   \n",
      "1997-05-20  0.086458  0.087500  0.081771  0.081771   0.081771   109344000   \n",
      "1997-05-21  0.081771  0.082292  0.068750  0.071354   0.071354   377064000   \n",
      "\n",
      "            Dividends  Stock_Splits  \n",
      "Date                                 \n",
      "1997-05-15        0.0           0.0  \n",
      "1997-05-16        0.0           0.0  \n",
      "1997-05-19        0.0           0.0  \n",
      "1997-05-20        0.0           0.0  \n",
      "1997-05-21        0.0           0.0  \n",
      "\n",
      "--- Aligning News and Stock Data by Date ---\n",
      "Original news_df rows at start of alignment process: 1407328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1032\\1747959785.py:60: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', format='mixed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_df rows after pd.to_datetime (before initial dropna): 1407328\n",
      "No rows dropped during initial date conversion and dropna.\n",
      "news_df rows after initial dropna: 1407328\n",
      "News dates forced to datetime and converted to timezone-naive UTC representation.\n",
      "No additional rows dropped after full datetime standardization.\n",
      "news_df rows after full datetime standardization: 1407328\n",
      "news_df['date'] dtype AFTER all timezone standardization: datetime64[ns]\n",
      "Final news_df rows before floor/set_index: 1407328\n",
      "Successfully extracted date_only column.\n",
      "news_df rows after setting index: 1407328\n",
      "stock_df rows after floor: 6846\n",
      "daily_news_headlines series length: 3955\n",
      "\n",
      "Combined Daily News Headlines (first 5 entries):\n",
      "date_only\n",
      "2009-02-14                         How Treasuries and ETFs Work\n",
      "2009-04-27    Update on the Luxury Sector: 2nd Quarter 2009 ...\n",
      "2009-04-29                               Going Against the Herd\n",
      "2009-05-22    Charles Sizemore Radio Interview Saturday Morning\n",
      "2009-05-27    JVA perks to 39% gain, SMCG ready, MRM to cont...\n",
      "Name: combined_headline, dtype: object\n",
      "\n",
      "Merged DataFrame shape after date alignment and headline aggregation: (2757, 9)\n",
      "\n",
      "First 5 rows of Merged data:\n",
      "              Open    High     Low   Close  Adj_Close     Volume  Dividends  \\\n",
      "2009-04-27  4.1940  4.2490  4.1105  4.1560     4.1560  194118000        0.0   \n",
      "2009-04-29  4.1495  4.1495  3.9630  3.9895     3.9895  194702000        0.0   \n",
      "2009-05-22  3.8050  3.8520  3.7510  3.7820     3.7820   69694000        0.0   \n",
      "2009-05-27  3.9255  3.9750  3.8375  3.8550     3.8550  111342000        0.0   \n",
      "2009-05-29  3.8860  3.9005  3.8200  3.8995     3.8995  101444000        0.0   \n",
      "\n",
      "            Stock_Splits                                  combined_headline  \n",
      "2009-04-27           0.0  Update on the Luxury Sector: 2nd Quarter 2009 ...  \n",
      "2009-04-29           0.0                             Going Against the Herd  \n",
      "2009-05-22           0.0  Charles Sizemore Radio Interview Saturday Morning  \n",
      "2009-05-27           0.0  JVA perks to 39% gain, SMCG ready, MRM to cont...  \n",
      "2009-05-29           0.0  In $7.60 UTA - New Chinese Listing - Travel St...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Starting Task 3: Correlation Analysis ---\")\n",
    "\n",
    "# --- Load News Data ---\n",
    "print(\"\\n--- Loading News Data ---\")\n",
    "try:\n",
    "    # Ensure this is the only place news_df is loaded/modified for now\n",
    "    news_df = pd.read_csv(NEWS_RAW_PATH) # Do NOT parse_dates here, handle it in next step\n",
    "    print(\"News data loaded successfully.\")\n",
    "    print(f\"News DataFrame Info (initial load): {news_df.info()}\")\n",
    "    print(\"\\nFirst 5 rows of News data (initial load):\")\n",
    "    print(news_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: News data file not found at {NEWS_RAW_PATH}.\")\n",
    "    sys.exit(\"Exiting: News data file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load news data: {e}\")\n",
    "    sys.exit(\"Exiting: News data loading failed.\")\n",
    "\n",
    "if news_df.empty:\n",
    "    sys.exit(\"Exiting: News DataFrame is empty after loading.\")\n",
    "\n",
    "\n",
    "# --- Load Stock Data (for a single ticker to start, e.g., AMZN) ---\n",
    "print(\"\\n--- Loading Stock Data (e.g., AMZN) ---\")\n",
    "ticker_to_analyze = 'AMZN' # Choose a specific ticker for correlation analysis\n",
    "stock_file_name = f\"{ticker_to_analyze}_historical_data.csv\"\n",
    "stock_file_path = os.path.join(STOCK_DATA_DIR, stock_file_name)\n",
    "\n",
    "try:\n",
    "    stock_df = pd.read_csv(stock_file_path, parse_dates=True, index_col='Date')\n",
    "    stock_df.columns = [col.replace(' ', '_') for col in stock_df.columns]\n",
    "    if 'Adj_Close' in stock_df.columns and 'Close' not in stock_df.columns:\n",
    "        stock_df['Close'] = stock_df['Adj_Close']\n",
    "    stock_df.dropna(inplace=True)\n",
    "    print(f\"Stock data for {ticker_to_analyze} loaded successfully.\")\n",
    "    print(\"Stock DataFrame Info:\")\n",
    "    stock_df.info()\n",
    "    print(\"\\nFirst 5 rows of Stock data:\")\n",
    "    print(stock_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: Stock data file not found at {stock_file_path}.\")\n",
    "    sys.exit(\"Exiting: Stock data file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load stock data: {e}\")\n",
    "    sys.exit(\"Exiting: Stock data loading failed.\")\n",
    "\n",
    "if stock_df.empty:\n",
    "    sys.exit(\"Exiting: Stock DataFrame is empty after loading.\")\n",
    "\n",
    "# --- Date Alignment ---\n",
    "print(\"\\n--- Aligning News and Stock Data by Date ---\")\n",
    "\n",
    "# Store original length for comparison\n",
    "original_total_news_rows = len(news_df)\n",
    "print(f\"Original news_df rows at start of alignment process: {original_total_news_rows}\")\n",
    "\n",
    "\n",
    "# 1. Process News DataFrame 'date' column\n",
    "# Step 1.1: Convert to datetime, coercing errors, and using 'mixed' format.\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', format='mixed')\n",
    "print(f\"news_df rows after pd.to_datetime (before initial dropna): {len(news_df)}\")\n",
    "\n",
    "\n",
    "# Step 1.2: IMMEDIATELY drop rows where date conversion failed (became NaT)\n",
    "# This is CRITICAL. Any NaT values will prevent .dt accessor from working.\n",
    "rows_before_initial_dropna = len(news_df)\n",
    "news_df.dropna(subset=['date'], inplace=True)\n",
    "rows_after_initial_dropna = len(news_df)\n",
    "if rows_after_initial_dropna < rows_before_initial_dropna:\n",
    "    print(f\"Dropped {rows_before_initial_dropna - rows_after_initial_dropna} rows due to unparseable dates after initial conversion.\")\n",
    "else:\n",
    "    print(\"No rows dropped during initial date conversion and dropna.\")\n",
    "print(f\"news_df rows after initial dropna: {len(news_df)}\")\n",
    "\n",
    "# --- CRITICAL FIX: Ensure dtype is datetime64[ns] after coercion and dropna ---\n",
    "# If news_df['date'] is still 'object' dtype, it means some values couldn't be converted\n",
    "# even after errors='coerce', or it's mixed with non-datetime objects.\n",
    "# We explicitly cast it. This might introduce new NaNs if there are unconvertible types.\n",
    "# We also want it to be timezone-naive (datetime64[ns]) before the timezone logic.\n",
    "try:\n",
    "    # First, ensure it's a generic datetime type, then convert to UTC and make naive\n",
    "    # Handle potentially timezone-aware data first before forcing naive.\n",
    "    if pd.api.types.is_datetime64_any_dtype(news_df['date']):\n",
    "        # If it's already a datetime type, check for timezone and standardize\n",
    "        if news_df['date'].dt.tz is not None:\n",
    "            news_df['date'] = news_df['date'].dt.tz_convert('UTC')\n",
    "            print(\"News dates were timezone-aware, converted to UTC.\")\n",
    "        else:\n",
    "            # If naive, localize to UTC (assuming they are implicitly UTC or local times to be treated as UTC)\n",
    "            news_df['date'] = news_df['date'].dt.tz_localize('UTC', errors='coerce')\n",
    "            print(\"News dates were timezone-naive, localized to UTC.\")\n",
    "        \n",
    "        # Finally, strip the timezone information to get a naive datetime object representing UTC date\n",
    "        news_df['date'] = news_df['date'].dt.tz_localize(None)\n",
    "        print(\"News dates converted to timezone-naive UTC representation for daily alignment.\")\n",
    "        \n",
    "    else:\n",
    "        # If it's not a datetime dtype yet, it must be 'object'. Try to convert again explicitly to datetime64[ns, UTC]\n",
    "        # and then make naive. This is a failsafe.\n",
    "        news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', utc=True)\n",
    "        news_df['date'] = news_df['date'].dt.tz_localize(None) # Then strip timezone\n",
    "        print(\"News dates forced to datetime and converted to timezone-naive UTC representation.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during explicit datetime type conversion: {e}\")\n",
    "    # If conversion still fails, drop rows that couldn't be converted\n",
    "    news_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Drop NaNs again after timezone standardization/explicit conversion in case new NaTs were introduced\n",
    "current_rows = len(news_df)\n",
    "news_df.dropna(subset=['date'], inplace=True)\n",
    "if len(news_df) < current_rows:\n",
    "    print(f\"Dropped {current_rows - len(news_df)} rows from news_df after full datetime standardization due to new NaTs.\")\n",
    "else:\n",
    "    print(\"No additional rows dropped after full datetime standardization.\")\n",
    "print(f\"news_df rows after full datetime standardization: {len(news_df)}\")\n",
    "\n",
    "# VERIFY DTYPE AGAIN BEFORE .dt.floor('D')\n",
    "print(f\"news_df['date'] dtype AFTER all timezone standardization: {news_df['date'].dtype}\")\n",
    "if not pd.api.types.is_datetime64_any_dtype(news_df['date']):\n",
    "    print(\"CRITICAL: 'date' column is NOT datetime dtype after standardization. Cannot proceed with .dt accessor.\")\n",
    "    sys.exit(\"Exiting: 'date' column not datetime type.\")\n",
    "print(f\"Final news_df rows before floor/set_index: {len(news_df)}\") # Final check before setting index\n",
    "\n",
    "\n",
    "# Step 1.5: Extract only the date part (YYYY-MM-DD) for daily alignment\n",
    "# This line should now work without AttributeError\n",
    "news_df['date_only'] = news_df['date'].dt.floor('D') # Floors to the start of the day\n",
    "print(\"Successfully extracted date_only column.\")\n",
    "\n",
    "# Step 1.6: Set 'date_only' as the index for news_df for merging\n",
    "news_df.set_index('date_only', inplace=True)\n",
    "news_df.sort_index(inplace=True)\n",
    "print(f\"news_df rows after setting index: {len(news_df)}\")\n",
    "\n",
    "\n",
    "# 2. Process Stock DataFrame Index\n",
    "stock_df.index = stock_df.index.floor('D') # Floors to the start of the day\n",
    "stock_df.sort_index(inplace=True)\n",
    "print(f\"stock_df rows after floor: {len(stock_df)}\")\n",
    "\n",
    "\n",
    "# 3. Aggregate news headlines by date (if multiple on same day) BEFORE merging\n",
    "if 'headline' not in news_df.columns:\n",
    "    print(\"CRITICAL ERROR: 'headline' column not found in news_df. Please check your news data.\")\n",
    "    sys.exit(\"Exiting: Missing 'headline' column.\")\n",
    "\n",
    "daily_news_headlines = news_df.groupby(news_df.index)['headline'].apply(lambda x: ' '.join(x)).rename('combined_headline')\n",
    "print(f\"daily_news_headlines series length: {len(daily_news_headlines)}\")\n",
    "\n",
    "print(\"\\nCombined Daily News Headlines (first 5 entries):\")\n",
    "print(daily_news_headlines.head())\n",
    "\n",
    "\n",
    "# 4. Merge aggregated daily news headlines with daily stock data\n",
    "merged_df = pd.merge(\n",
    "    stock_df,\n",
    "    daily_news_headlines,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged DataFrame shape after date alignment and headline aggregation: {merged_df.shape}\")\n",
    "print(\"\\nFirst 5 rows of Merged data:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "if merged_df.empty:\n",
    "    sys.exit(\"Exiting: Merged DataFrame is empty. No common dates found between news and stock data after alignment.\")\n",
    "\n",
    "# ... (rest of your sentiment analysis, correlation, visualization code remains the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464cfd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Sentiment Analysis on News Headlines ---\n",
      "\n",
      "Merged DataFrame with Sentiment Scores (last 5 rows):\n",
      "                  Open        High         Low       Close   Adj_Close  \\\n",
      "2020-06-05  122.225502  124.432503  121.856499  124.150002  124.150002   \n",
      "2020-06-08  125.010002  126.500000  124.366997  126.203003  126.203003   \n",
      "2020-06-09  126.472000  131.321503  126.250000  130.042999  130.042999   \n",
      "2020-06-10  132.250000  136.117493  131.313004  132.372498  132.372498   \n",
      "2020-06-11  130.175003  133.569000  126.811501  127.898003  127.898003   \n",
      "\n",
      "               Volume  Dividends  Stock_Splits  \\\n",
      "2020-06-05   66128000        0.0           0.0   \n",
      "2020-06-08   79414000        0.0           0.0   \n",
      "2020-06-09  103520000        0.0           0.0   \n",
      "2020-06-10   98920000        0.0           0.0   \n",
      "2020-06-11  116002000        0.0           0.0   \n",
      "\n",
      "                                            combined_headline  sentiment_score  \n",
      "2020-06-05  77 Biggest Movers From Yesterday Shares of sev...         0.156350  \n",
      "2020-06-08  JMP Securities Reinstates Market Perform on Na...         0.119351  \n",
      "2020-06-09  Shares of several energy companies are trading...        -0.017079  \n",
      "2020-06-10  Several of industrial companies are trading lo...        -0.040486  \n",
      "2020-06-11  Shares of several financial service companies ...        -0.010033  \n",
      "Sentiment analysis completed. Added 'sentiment_score' column. Null sentiment scores: 0\n",
      "\n",
      "Basic sentiment score distribution:\n",
      "count    2757.000000\n",
      "mean        0.096451\n",
      "std         0.053520\n",
      "min        -0.168701\n",
      "25%         0.067467\n",
      "50%         0.094791\n",
      "75%         0.121593\n",
      "max         1.000000\n",
      "Name: sentiment_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Sentiment Analysis ---\n",
    "print(\"\\n--- Performing Sentiment Analysis on News Headlines ---\")\n",
    "\n",
    "# Ensure 'combined_headline' column exists. It should from the previous step.\n",
    "if 'combined_headline' not in merged_df.columns:\n",
    "    print(\"CRITICAL ERROR: 'combined_headline' column not found in merged_df. Please check previous steps.\")\n",
    "    sys.exit(\"Exiting: Missing 'combined_headline' column for sentiment analysis.\")\n",
    "\n",
    "# Function to get sentiment polarity using TextBlob\n",
    "# TextBlob's polarity ranges from -1.0 (negative) to 1.0 (positive)\n",
    "def get_sentiment_polarity(text):\n",
    "    if pd.isna(text): # Handle NaN or missing text\n",
    "        return 0.0 # Assign a neutral score for missing headlines\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to each combined headline\n",
    "merged_df['sentiment_score'] = merged_df['combined_headline'].apply(get_sentiment_polarity)\n",
    "\n",
    "print(\"\\nMerged DataFrame with Sentiment Scores (last 5 rows):\")\n",
    "print(merged_df.tail())\n",
    "\n",
    "print(f\"Sentiment analysis completed. Added 'sentiment_score' column. Null sentiment scores: {merged_df['sentiment_score'].isnull().sum()}\")\n",
    "\n",
    "# Basic check on sentiment distribution\n",
    "print(\"\\nBasic sentiment score distribution:\")\n",
    "print(merged_df['sentiment_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adee121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Daily Stock Returns & Preparing Data for Correlation ---\n",
      "\n",
      "Daily Stock Returns (last 5 entries):\n",
      "2020-06-05    0.009103\n",
      "2020-06-08    0.016536\n",
      "2020-06-09    0.030427\n",
      "2020-06-10    0.017913\n",
      "2020-06-11   -0.033802\n",
      "Name: Daily_Return, dtype: float64\n",
      "\n",
      "Sentiment Scores in Correlation DataFrame (last 5 entries):\n",
      "2020-06-05    0.156350\n",
      "2020-06-08    0.119351\n",
      "2020-06-09   -0.017079\n",
      "2020-06-10   -0.040486\n",
      "2020-06-11   -0.010033\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Correlation DataFrame shape: (2756, 2)\n"
     ]
    }
   ],
   "source": [
    "# notebooks/03_Correlation_Analysis.ipynb - New Cell\n",
    "\n",
    "# --- Calculate Daily Stock Returns & Final Data Prep for Correlation ---\n",
    "print(\"\\n--- Calculating Daily Stock Returns & Preparing Data for Correlation ---\")\n",
    "\n",
    "# Calculate Daily Stock Returns\n",
    "# Use .pct_change() to get percentage change from previous day\n",
    "merged_df['Daily_Return'] = merged_df['Close'].pct_change()\n",
    "\n",
    "# For correlation, we need to ensure both 'Daily_Return' and 'sentiment_score'\n",
    "# are non-null and aligned.\n",
    "# The 'sentiment_score' was created from 'combined_headline' which is per-day,\n",
    "# so no further aggregation is needed for sentiment itself.\n",
    "# We will drop NaNs that result from .pct_change() (the first day will be NaN)\n",
    "# and any NaNs in sentiment_score (which should be 0.0 if handled by function).\n",
    "\n",
    "# Create the DataFrame specifically for correlation analysis\n",
    "correlation_df = merged_df[['Daily_Return', 'sentiment_score']].dropna()\n",
    "\n",
    "print(\"\\nDaily Stock Returns (last 5 entries):\")\n",
    "print(correlation_df['Daily_Return'].tail())\n",
    "\n",
    "print(\"\\nSentiment Scores in Correlation DataFrame (last 5 entries):\")\n",
    "print(correlation_df['sentiment_score'].tail())\n",
    "\n",
    "print(f\"\\nCorrelation DataFrame shape: {correlation_df.shape}\")\n",
    "if correlation_df.empty:\n",
    "    print(\"WARNING: Correlation DataFrame is empty after dropping NaNs. Cannot proceed with correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9252db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Correlation Analysis ---\n",
      "\n",
      "Pearson Correlation between Average Daily News Sentiment and AMZN Daily Returns: 0.0597\n",
      "Interpretation: Very weak or no linear correlation.\n",
      "Direction: Positive. Positive news sentiment tends to be associated with positive stock returns, and vice-versa.\n",
      "\n",
      "Important Note: Correlation does not imply causation!\n"
     ]
    }
   ],
   "source": [
    "# notebooks/03_Correlation_Analysis.ipynb - New Cell\n",
    "\n",
    "# --- Correlation Analysis ---\n",
    "print(\"\\n--- Performing Correlation Analysis ---\")\n",
    "\n",
    "if correlation_df.empty:\n",
    "    print(\"WARNING: Correlation DataFrame is empty. Skipping correlation calculation.\")\n",
    "else:\n",
    "    # Calculate Pearson correlation coefficient\n",
    "    # .corr() method directly on DataFrame computes pairwise correlation\n",
    "    correlation = correlation_df['Daily_Return'].corr(correlation_df['sentiment_score'])\n",
    "    \n",
    "    print(f\"\\nPearson Correlation between Average Daily News Sentiment and {ticker_to_analyze} Daily Returns: {correlation:.4f}\")\n",
    "\n",
    "    # Interpretation of correlation strength\n",
    "    abs_correlation = abs(correlation)\n",
    "    if abs_correlation >= 0.7:\n",
    "        print(\"Interpretation: Strong correlation.\")\n",
    "    elif abs_correlation >= 0.5:\n",
    "        print(\"Interpretation: Moderate correlation.\")\n",
    "    elif abs_correlation >= 0.3:\n",
    "        print(\"Interpretation: Weak correlation.\")\n",
    "    else:\n",
    "        print(\"Interpretation: Very weak or no linear correlation.\")\n",
    "\n",
    "    # Interpretation of direction\n",
    "    if correlation > 0:\n",
    "        print(\"Direction: Positive. Positive news sentiment tends to be associated with positive stock returns, and vice-versa.\")\n",
    "    elif correlation < 0:\n",
    "        print(\"Direction: Negative. Positive news sentiment tends to be associated with negative stock returns, and vice-versa.\")\n",
    "    else:\n",
    "        print(\"Direction: No linear relationship.\")\n",
    "\n",
    "    print(\"\\nImportant Note: Correlation does not imply causation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b38c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
