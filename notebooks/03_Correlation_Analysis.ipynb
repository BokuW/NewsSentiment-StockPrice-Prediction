{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afce3eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction' to sys.path for module imports.\n",
      "Project structure setup complete and config.py created/updated.\n",
      "Base Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\n",
      "News Raw Path: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\data\\raw_analyst_ratings.csv\n",
      "Stock Data Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\data\\yfinance_data\n",
      "Tickers to process: ['AAPL', 'AMZN', 'GOOG', 'META', 'NVDA', 'TSLA']\n",
      "\n",
      "--- Starting Task 3: Correlation Analysis ---\n",
      "\n",
      "--- Loading News Data ---\n",
      "News data loaded successfully.\n",
      "News DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1407328 entries, 0 to 1407327\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   Unnamed: 0  1407328 non-null  int64 \n",
      " 1   headline    1407328 non-null  object\n",
      " 2   url         1407328 non-null  object\n",
      " 3   publisher   1407328 non-null  object\n",
      " 4   date        1407328 non-null  object\n",
      " 5   stock       1407328 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 64.4+ MB\n",
      "\n",
      "First 5 rows of News data:\n",
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "                        date stock  \n",
      "0  2020-06-05 10:30:54-04:00     A  \n",
      "1  2020-06-03 10:45:20-04:00     A  \n",
      "2  2020-05-26 04:30:07-04:00     A  \n",
      "3  2020-05-22 12:45:06-04:00     A  \n",
      "4  2020-05-22 11:38:59-04:00     A  \n",
      "\n",
      "--- Loading Stock Data (e.g., AMZN) ---\n",
      "Stock data for AMZN loaded successfully.\n",
      "Stock DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 6846 entries, 1997-05-15 to 2024-07-30\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Open          6846 non-null   float64\n",
      " 1   High          6846 non-null   float64\n",
      " 2   Low           6846 non-null   float64\n",
      " 3   Close         6846 non-null   float64\n",
      " 4   Adj_Close     6846 non-null   float64\n",
      " 5   Volume        6846 non-null   int64  \n",
      " 6   Dividends     6846 non-null   float64\n",
      " 7   Stock_Splits  6846 non-null   float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 481.4 KB\n",
      "\n",
      "First 5 rows of Stock data:\n",
      "                Open      High       Low     Close  Adj_Close      Volume  \\\n",
      "Date                                                                        \n",
      "1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000   \n",
      "1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000   \n",
      "1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000   \n",
      "1997-05-20  0.086458  0.087500  0.081771  0.081771   0.081771   109344000   \n",
      "1997-05-21  0.081771  0.082292  0.068750  0.071354   0.071354   377064000   \n",
      "\n",
      "            Dividends  Stock_Splits  \n",
      "Date                                 \n",
      "1997-05-15        0.0           0.0  \n",
      "1997-05-16        0.0           0.0  \n",
      "1997-05-19        0.0           0.0  \n",
      "1997-05-20        0.0           0.0  \n",
      "1997-05-21        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "# notebooks/03_Correlation_Analysis.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob # For sentiment analysis\n",
    "\n",
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "# This block is crucial for importing from src.config\n",
    "def find_project_root(current_path):\n",
    "    \"\"\"\n",
    "    Finds the project root by looking for common project directories.\n",
    "    Assumes project_root contains 'src', 'data', and 'notebooks'.\n",
    "    \"\"\"\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path # Fallback if no specific root found\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables from your src.config\n",
    "from src.config import NEWS_RAW_PATH, STOCK_DATA_DIR, STOCK_TICKERS\n",
    "\n",
    "print(\"\\n--- Starting Task 3: Correlation Analysis ---\")\n",
    "\n",
    "# --- Load News Data ---\n",
    "print(\"\\n--- Loading News Data ---\")\n",
    "try:\n",
    "    news_df = pd.read_csv(NEWS_RAW_PATH, parse_dates=['date']) # Assuming 'date' column\n",
    "    print(\"News data loaded successfully.\")\n",
    "    print(\"News DataFrame Info:\")\n",
    "    news_df.info()\n",
    "    print(\"\\nFirst 5 rows of News data:\")\n",
    "    print(news_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: News data file not found at {NEWS_RAW_PATH}.\")\n",
    "    sys.exit(\"Exiting: News data file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load news data: {e}\")\n",
    "    sys.exit(\"Exiting: News data loading failed.\")\n",
    "\n",
    "if news_df.empty:\n",
    "    sys.exit(\"Exiting: News DataFrame is empty after loading.\")\n",
    "\n",
    "\n",
    "# --- Load Stock Data (for a single ticker to start, e.g., AMZN) ---\n",
    "# You can extend this to loop through all tickers later if needed for broader analysis.\n",
    "print(\"\\n--- Loading Stock Data (e.g., AMZN) ---\")\n",
    "ticker_to_analyze = 'AMZN' # Choose a specific ticker for correlation analysis\n",
    "stock_file_name = f\"{ticker_to_analyze}_historical_data.csv\"\n",
    "stock_file_path = os.path.join(STOCK_DATA_DIR, stock_file_name)\n",
    "\n",
    "try:\n",
    "    stock_df = pd.read_csv(stock_file_path, parse_dates=True, index_col='Date')\n",
    "    stock_df.columns = [col.replace(' ', '_') for col in stock_df.columns]\n",
    "    if 'Adj_Close' in stock_df.columns and 'Close' not in stock_df.columns:\n",
    "        stock_df['Close'] = stock_df['Adj_Close']\n",
    "    stock_df.dropna(inplace=True)\n",
    "    print(f\"Stock data for {ticker_to_analyze} loaded successfully.\")\n",
    "    print(\"Stock DataFrame Info:\")\n",
    "    stock_df.info()\n",
    "    print(\"\\nFirst 5 rows of Stock data:\")\n",
    "    print(stock_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: Stock data file not found at {stock_file_path}.\")\n",
    "    sys.exit(\"Exiting: Stock data file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load stock data: {e}\")\n",
    "    sys.exit(\"Exiting: Stock data loading failed.\")\n",
    "\n",
    "if stock_df.empty:\n",
    "    sys.exit(\"Exiting: Stock DataFrame is empty after loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc5a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aligning News and Stock Data by Date ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_14796\\3873166165.py:6: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', format='mixed')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m news_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Handle timezones:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# If a datetime is naive, localize it to UTC (assuming naive dates are implicitly UTC or local that we want to treat as UTC)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Then, convert all datetimes (now all timezone-aware) to UTC\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Finally, strip the timezone information to get a naive datetime object representing UTC date\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_convert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract only the date part (YYYY-MM-DD) for daily alignment\u001b[39;00m\n\u001b[0;32m     18\u001b[0m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Floors to the start of the day\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-1\\NewsSentiment-StockPrice-Prediction\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# --- Date Alignment ---\n",
    "print(\"\\n--- Aligning News and Stock Data by Date ---\")\n",
    "\n",
    "# 1. Process News DataFrame 'date' column\n",
    "# Step 1.1: Convert to datetime, coercing errors, and handling mixed formats\n",
    "# This is the critical step where parsing happens.\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', format='mixed')\n",
    "\n",
    "# Step 1.2: IMMEDIATELY drop rows where date conversion failed (became NaT)\n",
    "# This ensures that all remaining entries in 'date' are valid datetime objects.\n",
    "initial_rows = len(news_df)\n",
    "news_df.dropna(subset=['date'], inplace=True)\n",
    "if len(news_df) < initial_rows:\n",
    "    print(f\"Dropped {initial_rows - len(news_df)} rows from news_df due to unparseable dates.\")\n",
    "\n",
    "# Step 1.3: Handle timezones to ensure consistency (e.g., all UTC then naive)\n",
    "# Check if the 'date' column is timezone-aware\n",
    "if news_df['date'].dt.tz is not None:\n",
    "    # If it's already timezone-aware, convert it to UTC\n",
    "    news_df['date'] = news_df['date'].dt.tz_convert('UTC')\n",
    "    print(\"News dates were timezone-aware, converted to UTC.\")\n",
    "else:\n",
    "    # If it's timezone-naive, localize it to UTC.\n",
    "    # We assume naive timestamps in your raw data are implicitly UTC or local times\n",
    "    # that you want to treat as UTC for global alignment.\n",
    "    news_df['date'] = news_df['date'].dt.tz_localize('UTC', errors='coerce')\n",
    "    print(\"News dates were timezone-naive, localized to UTC.\")\n",
    "    # Drop NaNs again in case localization failed for some entries (less common but safe)\n",
    "    news_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Step 1.4: Convert all dates to timezone-naive (strip timezone) for daily alignment\n",
    "# This is safe now because we've ensured all are consistently UTC-aware before stripping.\n",
    "news_df['date'] = news_df['date'].dt.tz_localize(None)\n",
    "print(\"News dates converted to timezone-naive UTC representation for daily alignment.\")\n",
    "\n",
    "# Step 1.5: Extract only the date part (YYYY-MM-DD) for daily alignment\n",
    "news_df['date_only'] = news_df['date'].dt.floor('D') # Floors to the start of the day\n",
    "\n",
    "# Step 1.6: Set 'date_only' as the index for news_df for merging\n",
    "news_df.set_index('date_only', inplace=True)\n",
    "news_df.sort_index(inplace=True)\n",
    "\n",
    "# 2. Process Stock DataFrame Index\n",
    "# Ensure stock_df index is also datetime (already done by parse_dates=True, index_col='Date')\n",
    "# Convert stock_df index to date-only for daily alignment\n",
    "# Assuming stock data is already in a consistent timezone or timezone-naive and represents market close of that day\n",
    "stock_df.index = stock_df.index.floor('D') # Floors to the start of the day\n",
    "stock_df.sort_index(inplace=True)\n",
    "\n",
    "# 3. Aggregate news headlines by date (if multiple on same day) BEFORE merging\n",
    "# This step is crucial because we want one sentiment score per day.\n",
    "# We'll concatenate headlines for now; sentiment analysis will be applied to this concatenated string.\n",
    "if 'headline' not in news_df.columns:\n",
    "    print(\"CRITICAL ERROR: 'headline' column not found in news_df. Please check your news data.\")\n",
    "    sys.exit(\"Exiting: Missing 'headline' column.\")\n",
    "\n",
    "daily_news_headlines = news_df.groupby(news_df.index)['headline'].apply(lambda x: ' '.join(x)).rename('combined_headline')\n",
    "\n",
    "print(\"\\nCombined Daily News Headlines (first 5 entries):\")\n",
    "print(daily_news_headlines.head())\n",
    "\n",
    "\n",
    "# 4. Merge aggregated daily news headlines with daily stock data\n",
    "# Use an inner merge to keep only dates common to both datasets\n",
    "merged_df = pd.merge(\n",
    "    stock_df,\n",
    "    daily_news_headlines,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged DataFrame shape after date alignment and headline aggregation: {merged_df.shape}\")\n",
    "print(\"\\nFirst 5 rows of Merged data:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "if merged_df.empty:\n",
    "    sys.exit(\"Exiting: Merged DataFrame is empty. No common dates found between news and stock data after alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6337c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
